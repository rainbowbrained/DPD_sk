{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd7e169",
   "metadata": {},
   "source": [
    "\n",
    "Implement a neural network-based predistorter for the linearization of high power amplifier. \n",
    "Show by simulations how the activation function type (ReLU/sigmoid) and the number of network layers impact the nonlinear noise suppression.\n",
    "\n",
    "\n",
    "R. Hongyo, Y. Egashira, T. M. Hone and K. Yamaguchi, \"Deep Neural Network-Based Digital Predistorter for Doherty Power Amplifiers,\" in  in IEEE Microwave and Wireless Components Letters, vol. 29, no. 2, pp. 146-148, Feb. 2019, doi: 10.1109/LMWC.2018.2888955. \n",
    "\n",
    "[[publication link]](https://www.researchgate.net/publication/341539322_Convolutional_Neural_Network_for_Behavioral_Modeling_and_Predistortion_of_Wideband_Power_Amplifiers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Baseline grade (8/10) requirements:\n",
    "- Provide background material (motivation, theory, existing methods of solution)\n",
    "- Demonstrate your numerical implementation\n",
    "- Analyze results (validate your numerical solution, compare with theory and other existing methods of solution)\n",
    "- Answer questions\n",
    "        \n",
    "Higher grades (9,10) are usually given for some of the following:\n",
    "- An original, interesting project topic (understandable by a non-expert)\n",
    "- An original approach/solution\n",
    "- An extensive analysis of the topic / substantial amount of work / implementation involving advanced technologies\n",
    "- An outstanding presentation\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1018532",
   "metadata": {},
   "source": [
    "**SOURCES**\n",
    "1. https://github.com/lab-emi/OpenDPD\n",
    "https://arxiv.org/html/2401.08318v2/#S2\n",
    "\n",
    "2. disser https://open.fau.de/server/api/core/bitstreams/e5af6398-60b8-4f08-89df-c39203dd37c6/content\n",
    "\n",
    "2. book High performance Deep Learning based Digital Pre-distorters for RF Power Amplifiers\n",
    "https://vtechworks.lib.vt.edu/server/api/core/bitstreams/5e0ff5c4-124b-4f70-9b27-b259ffd89bd8/content\n",
    "\n",
    "3. paper Linearizing Power Amplifiers Using Digital Predistortion, EDA Tools and Test Hardware\n",
    "http://www.summittechmedia.com/highfreqelec/Apr04/HFE0404_Stapleton.pdf\n",
    "\n",
    "4. paper Deep Neural Network-Based Digital Pre-Distortion for High Baudrate Optical Coherent Transmission\n",
    "https://opg.optica.org/directpdfaccess/6ee808be-bdb2-4822-a1d907d1c0b1c715_469680/jlt-40-3-597.pdf?da=1&id=469680&seq=0&mobile=no\n",
    "\n",
    "5. paper Low-Complexity Adaptive Digital Predistortion with Meta-Learning based Neural Network\n",
    "https://cea.hal.science/cea-03837858/file/Low_Complexity_Digital_Predistorsion_based_on_Neural_Networks%20(1).pdf\n",
    "\n",
    "6. paper Bandwidth-Scalable Digital Predistortion of Active Phased Array Using Transfer Learning Neural Network https://vbn.aau.dk/ws/portalfiles/portal/536331830/Bandwidth_Scalable_Digital_Predistortion_of_Active_Phased_Array_Using_Transfer_Learning_Neural_Network.pdf \n",
    "\n",
    "7. activation functions\n",
    "https://www.v7labs.com/blog/neural-networks-activation-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702353aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb2702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5bfcc18",
   "metadata": {},
   "source": [
    "Project Steps\n",
    "1. Literature Review\n",
    "\n",
    "    Study Relevant Papers: Begin by thoroughly reviewing the paper titled \"Convolutional Neural Network for Behavioral Modeling and Predistortion of Wideband Power Amplifiers\" as well as other relevant literature on neural networks in predistortion.\n",
    "    Identify Key Concepts: Focus on understanding the architecture of neural networks used for predistortion, particularly the types of activation functions (ReLU, sigmoid) and their effects.\n",
    "\n",
    "2. Define Project Objectives\n",
    "\n",
    "    Objective: To create a neural network model that serves as a predistorter for HPAs and to evaluate how varying the activation function type and number of layers affects nonlinear noise suppression.\n",
    "\n",
    "3. Data Collection\n",
    "\n",
    "    Collect Data: Obtain datasets that represent the input-output characteristics of the high power amplifier you are modeling. This could involve measurements from experimental setups or simulations.\n",
    "    Preprocess Data: Clean and preprocess the data to ensure it is suitable for training a neural network. This may include normalization and splitting into training, validation, and test sets.\n",
    "\n",
    "4. Neural Network Design\n",
    "\n",
    "    Choose Framework: Select a deep learning framework (e.g., TensorFlow, PyTorch) for implementing the neural network.\n",
    "    Network Architecture:\n",
    "        Input Layer: Design an input layer that matches the dimensions of your data.\n",
    "        Hidden Layers: Experiment with different configurations:\n",
    "            Vary the number of hidden layers (e.g., 1 to 5 layers).\n",
    "            Choose between ReLU and sigmoid activation functions for each layer.\n",
    "        Output Layer: Design an output layer that predicts the desired output (predistorted signal).\n",
    "\n",
    "5. Implementation\n",
    "\n",
    "    Code the Model: Write code to implement the neural network architecture. Below is a basic example in Python using TensorFlow:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea39c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83c5dfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 20:38:39.480258: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-22 20:38:39.625047: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-22 20:38:39.671135: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-22 20:38:39.728294: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-22 20:38:41.275162: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_model(num_layers, activation_function):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(input_dim,)))\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        model.add(layers.Dense(units=64, activation=activation_function))\n",
    "    \n",
    "    model.add(layers.Dense(units=output_dim))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8af6c9",
   "metadata": {},
   "source": [
    "6. Training the Model\n",
    "\n",
    "    Train the Model: Train your model using the training dataset. Monitor performance using validation data to avoid overfitting.\n",
    "    Hyperparameter Tuning: Adjust learning rates, batch sizes, and epochs based on performance metrics.\n",
    "\n",
    "7. Simulation and Evaluation\n",
    "\n",
    "    Run Simulations: After training, run simulations to evaluate how well your model predicts the predistorted signal.\n",
    "    Compare Activation Functions: Analyze results from models using different activation functions (ReLU vs. sigmoid).\n",
    "    Vary Network Depth: Compare results from networks with varying numbers of layers.\n",
    "\n",
    "8. Analyze Results\n",
    "\n",
    "    Performance Metrics: Use metrics such as Mean Squared Error (MSE) or Signal-to-Noise Ratio (SNR) to quantify performance.\n",
    "    Visualization: Plot results to visualize how activation functions and network depth impact nonlinear noise suppression.\n",
    "\n",
    "9. Documentation\n",
    "\n",
    "    Document Findings: Compile your findings into a report detailing methodology, results, and conclusions drawn from simulations.\n",
    "    Future Work Suggestions: Suggest areas for further research or improvements based on your findings.\n",
    "\n",
    "10. Presentation\n",
    "\n",
    "    Prepare a presentation summarizing your project objectives, methodology, results, and conclusions for stakeholders or academic review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf94de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8bd420a",
   "metadata": {},
   "source": [
    "Motivation\n",
    "High power amplifiers (HPAs) are critical components in communication systems, particularly in wireless transmission, where they amplify signals to transmit over long distances. However, HPAs often exhibit nonlinear behavior, leading to signal distortion and spectral regrowth, which can severely degrade the performance of communication systems. To mitigate these issues, digital predistortion (DPD) techniques are employed to linearize the amplifier's output by pre-processing the input signal. The advent of deep learning and neural networks presents new opportunities to enhance the effectiveness of DPD methods, enabling more accurate modeling of amplifier behavior and improved linearization performance.\n",
    "\n",
    "\n",
    "Theory\n",
    "The core principle behind predistortion is to apply a nonlinear transformation to the input signal that counteracts the nonlinearities introduced by the amplifier. This transformation can be modeled using various mathematical approaches, including Volterra series and polynomial models. However, these traditional methods may struggle with complex nonlinearities present in modern HPAs. Neural networks, particularly deep neural networks (DNNs), offer a flexible framework for modeling such nonlinear relationships due to their ability to learn complex functions from data. By training on input-output pairs from the amplifier, a neural network can learn to predict the necessary predistortion that will result in a linear output. Key components involved in this process include:\n",
    "\n",
    "    Activation Functions: Non-linear functions such as ReLU (Rectified Linear Unit) and sigmoid are crucial for introducing non-linearity into the neural network. They enable the network to learn complex patterns and relationships within the data 3\n",
    "    5\n",
    "    .\n",
    "    Loss Function: A loss function quantifies how well the neural network's predictions match the desired output. Common choices for DPD applications include Mean Squared Error (MSE) or custom loss functions designed to minimize distortion metrics.\n",
    "    Training Process: The training involves adjusting the weights of the neural network through backpropagation based on the error calculated from the loss function.\n",
    "\n",
    "\n",
    "\n",
    "Existing Methods of Solution\n",
    "\n",
    "    Traditional DPD Techniques:\n",
    "        Volterra Series: This method models nonlinear systems using a series expansion that captures both memory effects and nonlinearity but can become computationally intensive.\n",
    "        Polynomial Models: These models use polynomial equations to approximate the amplifier's transfer characteristics but may not generalize well across different operating conditions.\n",
    "    Neural Network Approaches:\n",
    "        Direct Learning Architectures: These architectures directly model the relationship between input and output signals without intermediate transformations 2\n",
    "        . However, they may require extensive training data.\n",
    "        Indirect Learning Architectures (ILA): In ILA, separate networks are used to model amplitude and phase distortions, allowing for more targeted corrections 2\n",
    "        . This approach has shown improved performance over direct models.\n",
    "        Walsh-Domain Neural Networks: Recent advancements have adapted neural networks to operate in alternative domains (e.g., Walsh domain), allowing for parallel processing and potentially enhanced performance in capturing time-frequency characteristics of signals 1\n",
    "        .\n",
    "    Meta-Learning Approaches:\n",
    "        Recent studies have explored meta-learning techniques that allow neural networks to adapt quickly to changing conditions in HPAs, enhancing their robustness and efficiency 2\n",
    "        . This involves training models on multiple tasks or configurations simultaneously, enabling them to generalize better across different scenarios.\n",
    "    Knowledge Distillation:\n",
    "        A novel approach involves using a pre-trained model (teacher) to guide a simpler model (student) through a process known as Knowledge Distillation. This technique allows for efficient learning by transferring knowledge from a complex model to a more streamlined one without significant loss in performance 1\n",
    "        ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2984c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d510a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd37da68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e27ad3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
